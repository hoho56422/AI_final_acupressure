{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "加cc.zh.100.ve"
      ],
      "metadata": {
        "id": "v9_U3cWLF3GC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get -y install fonts-noto-cjk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdGyFnv6z872",
        "outputId": "f6560a59-2f94-4f98-986c-8e5570f02a04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  fonts-noto-cjk-extra\n",
            "The following NEW packages will be installed:\n",
            "  fonts-noto-cjk\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 61.2 MB of archives.\n",
            "After this operation, 93.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-cjk all 1:20220127+repack1-1 [61.2 MB]\n",
            "Fetched 61.2 MB in 4s (17.5 MB/s)\n",
            "Selecting previously unselected package fonts-noto-cjk.\n",
            "(Reading database ... 126109 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-noto-cjk_1%3a20220127+repack1-1_all.deb ...\n",
            "Unpacking fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Setting up fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import jieba\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import rcParams\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "font_path = \"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\"\n",
        "my_font = FontProperties(fname=font_path)\n",
        "rcParams['font.family'] = my_font.get_name()\n",
        "from opencc import OpenCC\n",
        "cc = OpenCC('t2s')\n",
        "\n",
        "def convert_to_simplified(text):\n",
        "    return cc.convert(text)\n",
        "\n",
        "# ========== 加載 FastText 嵌入 ==========\n",
        "def load_fasttext_embedding(path, vocab, embed_dim):\n",
        "    print(\"Loading filtered fastText embedding...\")\n",
        "\n",
        "    embedding_dict = {}\n",
        "    needed_words = set(vocab.keys())\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        header = f.readline()\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            if len(values) != embed_dim + 1:\n",
        "                continue\n",
        "            word = values[0]\n",
        "            if word in needed_words:\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                embedding_dict[word] = vector\n",
        "\n",
        "\n",
        "    matrix = np.zeros((len(vocab), embed_dim))\n",
        "    unk_count = 0\n",
        "    total_count = len(vocab) - 2\n",
        "\n",
        "    for word, idx in vocab.items():\n",
        "        if word in embedding_dict:\n",
        "            matrix[idx] = embedding_dict[word]\n",
        "        else:\n",
        "            matrix[idx] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
        "            if word not in [\"<PAD>\", \"<UNK>\"]:\n",
        "                unk_count += 1\n",
        "\n",
        "    print(f\"OOV rate: {unk_count / total_count:.4f}\")\n",
        "    return torch.tensor(matrix, dtype=torch.float)\n",
        "\n",
        "\n",
        "def build_vocab(texts, min_freq=1):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        simplified = cc.convert(text)\n",
        "        words = jieba.lcut(simplified)\n",
        "        word_counts.update(words)\n",
        "\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    idx = 2\n",
        "    for word, freq in word_counts.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "    return vocab\n",
        "\n",
        "def text_to_indices(text, vocab, max_len=50):\n",
        "    words = jieba.lcut(text)\n",
        "    indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in words]\n",
        "    if len(indices) < max_len:\n",
        "        indices += [vocab[\"<PAD>\"]] * (max_len - len(indices))\n",
        "    else:\n",
        "        indices = indices[:max_len]\n",
        "    return indices\n",
        "\n",
        "class SymptomDataset(Dataset):\n",
        "    def __init__(self, data, vocab, label_to_idx, max_len=50):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.label_to_idx = label_to_idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        label = self.data[idx][\"label\"]\n",
        "        indices = text_to_indices(text, self.vocab, self.max_len)\n",
        "        return torch.tensor(indices, dtype=torch.long), torch.tensor(self.label_to_idx[label], dtype=torch.long)\n",
        "\n",
        "class SymptomGRUAttention(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, num_classes, dropout_rate=0.5):\n",
        "        super(SymptomGRUAttention, self).__init__()\n",
        "        num_embeddings, embed_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        gru_out = self.dropout(gru_out)\n",
        "        attn_weights = torch.softmax(self.attn(gru_out), dim=1)\n",
        "        context = torch.sum(attn_weights * gru_out, dim=1)\n",
        "        context = self.dropout(context)\n",
        "        out = self.classifier(context)\n",
        "        return out, attn_weights\n",
        "\n",
        "# ==========畫圖 ==========\n",
        "def plot_confusion_matrix(y_true, y_pred, labels):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xticks(rotation=45, fontproperties=my_font)\n",
        "    plt.yticks(fontproperties=my_font)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10, patience=3):\n",
        "    model.train()\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for texts, labels in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        val_loss = evaluate_loss(model, val_loader, criterion, device)\n",
        "        # print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "    return model\n",
        "\n",
        "def evaluate_loss(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs, _ = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "def evaluate_model(model, data_loader, device, idx_to_label):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in data_loader:\n",
        "            texts = texts.to(device)\n",
        "            outputs, _ = model(texts)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    used_indices = sorted(set(all_preds + all_labels))\n",
        "    used_labels = [idx_to_label[i] for i in used_indices]\n",
        "    # print(\"\\n--- GRU+Attention Evaluation Report ---\")\n",
        "    # print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
        "    # print(\"Precision:\", precision_score(all_labels, all_preds, average='macro', zero_division=0))\n",
        "    # print(\"Recall:\", recall_score(all_labels, all_preds, average='macro', zero_division=0))\n",
        "    # print(\"F1 Score:\", f1_score(all_labels, all_preds, average='macro', zero_division=0))\n",
        "    # print(classification_report(all_labels, all_preds, labels=used_indices, target_names=used_labels, zero_division=0))\n",
        "    #下面可看confusion matrix\n",
        "    # plot_confusion_matrix(all_labels, all_preds, [idx_to_label[i] for i in range(len(idx_to_label))])\n",
        "\n",
        "\n",
        "def predict_with_scores(model, text, vocab, idx_to_label, max_len=50, device='cpu', threshold=0.2):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        indices = text_to_indices(text, vocab, max_len)\n",
        "        indices = torch.tensor([indices], dtype=torch.long).to(device)\n",
        "        outputs, attn_weights = model(indices)\n",
        "        probs = torch.softmax(outputs, dim=1)[0]\n",
        "        confidence, pred_idx = torch.max(probs, dim=0)\n",
        "        pred_label = idx_to_label[pred_idx.item()]\n",
        "        score_dict = {idx_to_label[i]: float(probs[i]) for i in range(len(probs))}\n",
        "\n",
        "        words = jieba.lcut(text)[:max_len]\n",
        "        attn_weights = attn_weights.squeeze().cpu().numpy()[:len(words)]\n",
        "        print(\"\\n注意力權重：\")\n",
        "        for word, weight in zip(words, attn_weights):\n",
        "            print(f\"{word}: {weight:.4f}\")\n",
        "\n",
        "        return {\n",
        "            \"prediction\": pred_label,\n",
        "            \"confidence\": float(confidence),\n",
        "            \"scores\": score_dict\n",
        "        }\n",
        "\n",
        "def run_inference_with_score(text, model, vocab, idx_to_label, max_len, device, threshold=0.35):\n",
        "    result = predict_with_scores(model, text, vocab, idx_to_label, max_len, device)\n",
        "    prediction = result[\"prediction\"] if result[\"confidence\"] >= threshold else \"無法判斷\"\n",
        "\n",
        "    print(f\"\\n【輸入症狀】：{text}\")\n",
        "    print(f\"【預測結果】：{prediction}\")\n",
        "    # print(f\"【模型信心】：{result['confidence']:.4f}\")\n",
        "    # print(\"【各類別分數】：\")\n",
        "    sorted_scores = sorted(result[\"scores\"].items(), key=lambda x: x[1], reverse=True)\n",
        "    for label, score in sorted_scores:\n",
        "        print(f\"{label:<15} : {score:.4f}\")\n",
        "\n",
        "def main():\n",
        "    max_len = 50\n",
        "    embed_dim = 300\n",
        "    hidden_dim = 16\n",
        "    num_epochs = 10\n",
        "    batch_size = 16\n",
        "    dropout_rate = 0.3\n",
        "    weight_decay = 1e-4\n",
        "    patience = 3\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    with open(\"sympton_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    texts = [item[\"text\"] for item in dataset]\n",
        "    labels = [item[\"label\"] for item in dataset]\n",
        "\n",
        "    # print(\"類別分佈：\", Counter(labels))\n",
        "\n",
        "    vocab = build_vocab(texts, min_freq=2)\n",
        "    label_set = sorted(set(labels))\n",
        "    label_to_idx = {label: idx for idx, label in enumerate(label_set)}\n",
        "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
        "\n",
        "    class_counts = Counter(labels)\n",
        "    class_weights = torch.tensor([1.0 / class_counts[label] for label in label_set], dtype=torch.float).to(device)\n",
        "\n",
        "    embedding_matrix = load_fasttext_embedding(\"zh_wiki_fasttext_300.txt\", vocab, embed_dim)\n",
        "\n",
        "\n",
        "    full_dataset = SymptomDataset(dataset, vocab, label_to_idx, max_len)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = SymptomGRUAttention(\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_classes=len(label_to_idx),\n",
        "        dropout_rate=dropout_rate\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0025, weight_decay=weight_decay)\n",
        "\n",
        "    model = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience)\n",
        "    evaluate_model(model, val_loader, device, idx_to_label)\n",
        "    with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
        "    with open(\"label_to_idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(label_to_idx, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN0RwWx0FtBt",
        "outputId": "cb3cb36d-b049-4737-d811-89eae18d2b95",
        "collapsed": true
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading filtered fastText embedding...\n",
            "OOV rate: 0.0845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 第二區塊：playground ======\n",
        "import torch, json, jieba, numpy as np\n",
        "from opencc import OpenCC\n",
        "\n",
        "class SymptomGRUAttention(torch.nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, num_classes, dropout_rate=0.3):\n",
        "        super(SymptomGRUAttention, self).__init__()\n",
        "        num_embeddings, embed_dim = embedding_matrix.shape\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
        "        self.gru = torch.nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.attn = torch.nn.Linear(hidden_dim * 2, 1)\n",
        "        self.classifier = torch.nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        gru_out = self.dropout(gru_out)\n",
        "        attn_weights = torch.softmax(self.attn(gru_out), dim=1)\n",
        "        context = torch.sum(attn_weights * gru_out, dim=1)\n",
        "        context = self.dropout(context)\n",
        "        out = self.classifier(context)\n",
        "        return out, attn_weights\n",
        "\n",
        "\n",
        "cc = OpenCC('t2s')\n",
        "def text_to_indices(text, vocab, max_len=50):\n",
        "    words = jieba.lcut(cc.convert(text))\n",
        "    indices = [vocab.get(w, vocab[\"<UNK>\"]) for w in words]\n",
        "    return indices[:max_len] + [vocab[\"<PAD>\"]] * max(0, max_len - len(indices))\n",
        "\n",
        "\n",
        "with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab = json.load(f)\n",
        "with open(\"label_to_idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    label_to_idx = json.load(f)\n",
        "idx_to_label = {int(v): k for k, v in label_to_idx.items()}\n",
        "\n",
        "def load_embedding_matrix(path, vocab, embed_dim):\n",
        "    embedding_dict = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        f.readline()  # skip header\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != embed_dim + 1: continue\n",
        "            word, vector = parts[0], np.array(parts[1:], dtype='float32')\n",
        "            if word in vocab:\n",
        "                embedding_dict[word] = vector\n",
        "    matrix = np.zeros((len(vocab), embed_dim))\n",
        "    for word, idx in vocab.items():\n",
        "        matrix[idx] = embedding_dict.get(word, np.random.normal(scale=0.6, size=(embed_dim,)))\n",
        "    return torch.tensor(matrix, dtype=torch.float)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "embed_dim = 300\n",
        "hidden_dim = 16\n",
        "max_len = 50\n",
        "embedding_matrix = load_embedding_matrix(\"zh_wiki_fasttext_300.txt\", vocab, embed_dim)\n",
        "\n",
        "model = SymptomGRUAttention(embedding_matrix, hidden_dim, len(label_to_idx)).to(device)\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def predict_symptom(text, threshold=0.35):\n",
        "    indices = text_to_indices(text, vocab, max_len)\n",
        "    input_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs, attn_weights = model(input_tensor)\n",
        "        probs = torch.softmax(outputs, dim=1)[0]\n",
        "        pred_idx = torch.argmax(probs).item()\n",
        "        confidence = probs[pred_idx].item()\n",
        "        label = idx_to_label[pred_idx] if confidence >= threshold else \"無法判斷，請再輸入一次\"\n",
        "        top3 = sorted({idx_to_label[i]: float(p) for i, p in enumerate(probs)}.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "        return {\n",
        "            \"輸入\": text,\n",
        "            \"預測結果\": label,\n",
        "            \"信心分數\": round(confidence, 4),\n",
        "            \"前三高分類別\": top3\n",
        "        }\n",
        "\n",
        "#INPUT 在這\n",
        "test_text = \"我想下課\"\n",
        "result = predict_symptom(test_text)\n",
        "print(f\"\\n【輸入】：{result['輸入']}\")\n",
        "print(f\"【預測結果】：{result['預測結果']}\")\n",
        "# print(\"【前三分類分數】：\")\n",
        "# for label, score in result[\"前三高分類別\"]:\n",
        "#     print(f\"{label:<10}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "702nyvGoq_XN",
        "outputId": "5f21ec41-3fe4-4dd1-c468-fadb98f67243"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "【輸入】：我想下課\n",
            "【預測結果】：無法判斷，請再輸入一次（信心分數：0.3167）\n",
            "【前三分類分數】：\n",
            "經痛        : 0.3167\n",
            "水腫        : 0.1669\n",
            "壓力        : 0.0807\n"
          ]
        }
      ]
    }
  ]
}