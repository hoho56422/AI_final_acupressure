{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9_U3cWLF3GC"
      },
      "source": [
        "加cc.zh.100.ve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdGyFnv6z872",
        "outputId": "f6560a59-2f94-4f98-986c-8e5570f02a04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt-get' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jieba in c:\\python312\\lib\\site-packages (0.42.1)\n",
            "Requirement already satisfied: torch in c:\\python312\\lib\\site-packages (2.7.0)\n",
            "Requirement already satisfied: jieba in c:\\python312\\lib\\site-packages (0.42.1)\n",
            "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in c:\\python312\\lib\\site-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in c:\\python312\\lib\\site-packages (3.9.2)\n",
            "Requirement already satisfied: seaborn in c:\\python312\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\python312\\lib\\site-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python312\\lib\\site-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in c:\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python312\\lib\\site-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\python312\\lib\\site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch) (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install fonts-noto-cjk\n",
        "!pip install jieba\n",
        "!pip install torch jieba numpy scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yN0RwWx0FtBt",
        "outputId": "cb3cb36d-b049-4737-d811-89eae18d2b95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dumping model to file cache C:\\Users\\annie\\AppData\\Local\\Temp\\jieba.cache\n",
            "Loading model cost 0.655 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading filtered fastText embedding...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'zh_wiki_fasttext_300.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 280\u001b[39m\n\u001b[32m    277\u001b[39m         json.dump(label_to_idx, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m)\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 252\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    249\u001b[39m class_counts = Counter(labels)\n\u001b[32m    250\u001b[39m class_weights = torch.tensor([\u001b[32m1.0\u001b[39m / class_counts[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m label_set], dtype=torch.float).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m embedding_matrix = \u001b[43mload_fasttext_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzh_wiki_fasttext_300.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m full_dataset = SymptomDataset(dataset, vocab, label_to_idx, max_len)\n\u001b[32m    256\u001b[39m train_size = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.8\u001b[39m * \u001b[38;5;28mlen\u001b[39m(full_dataset))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mload_fasttext_embedding\u001b[39m\u001b[34m(path, vocab, embed_dim)\u001b[39m\n\u001b[32m     21\u001b[39m embedding_dict = {}\n\u001b[32m     22\u001b[39m needed_words = \u001b[38;5;28mset\u001b[39m(vocab.keys())\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mignore\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     25\u001b[39m     header = f.readline()\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'zh_wiki_fasttext_300.txt'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import jieba\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import rcParams\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "\n",
        "\n",
        "# ========== 加載 FastText 嵌入 ==========\n",
        "def load_fasttext_embedding(path, vocab, embed_dim):\n",
        "    print(\"Loading filtered fastText embedding...\")\n",
        "\n",
        "    embedding_dict = {}\n",
        "    needed_words = set(vocab.keys())\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        header = f.readline()\n",
        "        for line in f:\n",
        "            values = line.strip().split()\n",
        "            if len(values) != embed_dim + 1:\n",
        "                continue\n",
        "            word = values[0]\n",
        "            if word in needed_words:\n",
        "                vector = np.asarray(values[1:], dtype='float32')\n",
        "                embedding_dict[word] = vector\n",
        "\n",
        "\n",
        "    matrix = np.zeros((len(vocab), embed_dim))\n",
        "    unk_count = 0\n",
        "    total_count = len(vocab) - 2\n",
        "\n",
        "    for word, idx in vocab.items():\n",
        "        if word in embedding_dict:\n",
        "            matrix[idx] = embedding_dict[word]\n",
        "        else:\n",
        "            matrix[idx] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
        "            if word not in [\"<PAD>\", \"<UNK>\"]:\n",
        "                unk_count += 1\n",
        "\n",
        "    print(f\"OOV rate: {unk_count / total_count:.4f}\")\n",
        "    return torch.tensor(matrix, dtype=torch.float)\n",
        "\n",
        "\n",
        "def build_vocab(texts, min_freq=1):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        \n",
        "        words = jieba.lcut(text)\n",
        "        word_counts.update(words)\n",
        "\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    idx = 2\n",
        "    for word, freq in word_counts.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "    return vocab\n",
        "\n",
        "def text_to_indices(text, vocab, max_len=50):\n",
        "    words = jieba.lcut(text)\n",
        "    indices = [vocab.get(word, vocab[\"<UNK>\"]) for word in words]\n",
        "    if len(indices) < max_len:\n",
        "        indices += [vocab[\"<PAD>\"]] * (max_len - len(indices))\n",
        "    else:\n",
        "        indices = indices[:max_len]\n",
        "    return indices\n",
        "\n",
        "class SymptomDataset(Dataset):\n",
        "    def __init__(self, data, vocab, label_to_idx, max_len=50):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.label_to_idx = label_to_idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        label = self.data[idx][\"label\"]\n",
        "        indices = text_to_indices(text, self.vocab, self.max_len)\n",
        "        return torch.tensor(indices, dtype=torch.long), torch.tensor(self.label_to_idx[label], dtype=torch.long)\n",
        "\n",
        "class SymptomGRUAttention(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, num_classes, dropout_rate=0.5):\n",
        "        super(SymptomGRUAttention, self).__init__()\n",
        "        num_embeddings, embed_dim = embedding_matrix.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        gru_out = self.dropout(gru_out)\n",
        "        attn_weights = torch.softmax(self.attn(gru_out), dim=1)\n",
        "        context = torch.sum(attn_weights * gru_out, dim=1)\n",
        "        context = self.dropout(context)\n",
        "        out = self.classifier(context)\n",
        "        return out, attn_weights\n",
        "\n",
        "# ==========畫圖 ==========\n",
        "def plot_confusion_matrix(y_true, y_pred, labels):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xticks(rotation=45, fontproperties=my_font)\n",
        "    plt.yticks(fontproperties=my_font)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10, patience=3):\n",
        "    model.train()\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for texts, labels in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        val_loss = evaluate_loss(model, val_loader, criterion, device)\n",
        "        # print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "    return model\n",
        "\n",
        "def evaluate_loss(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs, _ = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "def evaluate_model(model, data_loader, device, idx_to_label):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in data_loader:\n",
        "            texts = texts.to(device)\n",
        "            outputs, _ = model(texts)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    used_indices = sorted(set(all_preds + all_labels))\n",
        "    used_labels = [idx_to_label[i] for i in used_indices]\n",
        "    # print(\"\\n--- GRU+Attention Evaluation Report ---\")\n",
        "    # print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
        "    # print(\"Precision:\", precision_score(all_labels, all_preds, average='macro', zero_division=0))\n",
        "    # print(\"Recall:\", recall_score(all_labels, all_preds, average='macro', zero_division=0))\n",
        "    # print(\"F1 Score:\", f1_score(all_labels, all_preds, average='macro', zero_division=0))\n",
        "    # print(classification_report(all_labels, all_preds, labels=used_indices, target_names=used_labels, zero_division=0))\n",
        "    #下面可看confusion matrix\n",
        "    # plot_confusion_matrix(all_labels, all_preds, [idx_to_label[i] for i in range(len(idx_to_label))])\n",
        "\n",
        "\n",
        "def predict_with_scores(model, text, vocab, idx_to_label, max_len=50, device='cpu', threshold=0.2):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        indices = text_to_indices(text, vocab, max_len)\n",
        "        indices = torch.tensor([indices], dtype=torch.long).to(device)\n",
        "        outputs, attn_weights = model(indices)\n",
        "        probs = torch.softmax(outputs, dim=1)[0]\n",
        "        confidence, pred_idx = torch.max(probs, dim=0)\n",
        "        pred_label = idx_to_label[pred_idx.item()]\n",
        "        score_dict = {idx_to_label[i]: float(probs[i]) for i in range(len(probs))}\n",
        "\n",
        "        words = jieba.lcut(text)[:max_len]\n",
        "        attn_weights = attn_weights.squeeze().cpu().numpy()[:len(words)]\n",
        "        print(\"\\n注意力權重：\")\n",
        "        for word, weight in zip(words, attn_weights):\n",
        "            print(f\"{word}: {weight:.4f}\")\n",
        "\n",
        "        return {\n",
        "            \"prediction\": pred_label,\n",
        "            \"confidence\": float(confidence),\n",
        "            \"scores\": score_dict\n",
        "        }\n",
        "\n",
        "def run_inference_with_score(text, model, vocab, idx_to_label, max_len, device, threshold=0.35):\n",
        "    result = predict_with_scores(model, text, vocab, idx_to_label, max_len, device)\n",
        "    prediction = result[\"prediction\"] if result[\"confidence\"] >= threshold else \"無法判斷\"\n",
        "\n",
        "    print(f\"\\n【輸入症狀】：{text}\")\n",
        "    print(f\"【預測結果】：{prediction}\")\n",
        "    # print(f\"【模型信心】：{result['confidence']:.4f}\")\n",
        "    # print(\"【各類別分數】：\")\n",
        "    sorted_scores = sorted(result[\"scores\"].items(), key=lambda x: x[1], reverse=True)\n",
        "    for label, score in sorted_scores:\n",
        "        print(f\"{label:<15} : {score:.4f}\")\n",
        "\n",
        "def main():\n",
        "    max_len = 50\n",
        "    embed_dim = 300\n",
        "    hidden_dim = 16\n",
        "    num_epochs = 10\n",
        "    batch_size = 16\n",
        "    dropout_rate = 0.3\n",
        "    weight_decay = 1e-4\n",
        "    patience = 3\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    with open(\"sympton_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    texts = [item[\"text\"] for item in dataset]\n",
        "    labels = [item[\"label\"] for item in dataset]\n",
        "\n",
        "    # print(\"類別分佈：\", Counter(labels))\n",
        "\n",
        "    vocab = build_vocab(texts, min_freq=2)\n",
        "    label_set = sorted(set(labels))\n",
        "    label_to_idx = {label: idx for idx, label in enumerate(label_set)}\n",
        "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
        "\n",
        "    class_counts = Counter(labels)\n",
        "    class_weights = torch.tensor([1.0 / class_counts[label] for label in label_set], dtype=torch.float).to(device)\n",
        "\n",
        "    embedding_matrix = load_fasttext_embedding(\"zh_wiki_fasttext_300.txt\", vocab, embed_dim)\n",
        "\n",
        "\n",
        "    full_dataset = SymptomDataset(dataset, vocab, label_to_idx, max_len)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = SymptomGRUAttention(\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_classes=len(label_to_idx),\n",
        "        dropout_rate=dropout_rate\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0025, weight_decay=weight_decay)\n",
        "\n",
        "    model = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, patience)\n",
        "    evaluate_model(model, val_loader, device, idx_to_label)\n",
        "    with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
        "    with open(\"label_to_idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(label_to_idx, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "702nyvGoq_XN",
        "outputId": "5f21ec41-3fe4-4dd1-c468-fadb98f67243"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'vocab.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     28\u001b[39m     indices = [vocab.get(w, vocab[\u001b[33m\"\u001b[39m\u001b[33m<UNK>\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m indices[:max_len] + [vocab[\u001b[33m\"\u001b[39m\u001b[33m<PAD>\u001b[39m\u001b[33m\"\u001b[39m]] * \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, max_len - \u001b[38;5;28mlen\u001b[39m(indices))\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvocab.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     33\u001b[39m     vocab = json.load(f)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlabel_to_idx.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'vocab.json'"
          ]
        }
      ],
      "source": [
        "# ====== 第二區塊：playground ======\n",
        "import torch, json, jieba, numpy as np\n",
        "\n",
        "\n",
        "class SymptomGRUAttention(torch.nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, num_classes, dropout_rate=0.3):\n",
        "        super(SymptomGRUAttention, self).__init__()\n",
        "        num_embeddings, embed_dim = embedding_matrix.shape\n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n",
        "        self.gru = torch.nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.attn = torch.nn.Linear(hidden_dim * 2, 1)\n",
        "        self.classifier = torch.nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        gru_out = self.dropout(gru_out)\n",
        "        attn_weights = torch.softmax(self.attn(gru_out), dim=1)\n",
        "        context = torch.sum(attn_weights * gru_out, dim=1)\n",
        "        context = self.dropout(context)\n",
        "        out = self.classifier(context)\n",
        "        return out, attn_weights\n",
        "\n",
        "\n",
        "def text_to_indices(text, vocab, max_len=50):\n",
        "    words = jieba.lcut(text)\n",
        "    indices = [vocab.get(w, vocab[\"<UNK>\"]) for w in words]\n",
        "    return indices[:max_len] + [vocab[\"<PAD>\"]] * max(0, max_len - len(indices))\n",
        "\n",
        "\n",
        "with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab = json.load(f)\n",
        "with open(\"label_to_idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    label_to_idx = json.load(f)\n",
        "idx_to_label = {int(v): k for k, v in label_to_idx.items()}\n",
        "\n",
        "def load_embedding_matrix(path, vocab, embed_dim):\n",
        "    embedding_dict = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        f.readline()  # skip header\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != embed_dim + 1: continue\n",
        "            word, vector = parts[0], np.array(parts[1:], dtype='float32')\n",
        "            if word in vocab:\n",
        "                embedding_dict[word] = vector\n",
        "    matrix = np.zeros((len(vocab), embed_dim))\n",
        "    for word, idx in vocab.items():\n",
        "        matrix[idx] = embedding_dict.get(word, np.random.normal(scale=0.6, size=(embed_dim,)))\n",
        "    return torch.tensor(matrix, dtype=torch.float)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "embed_dim = 300\n",
        "hidden_dim = 16\n",
        "max_len = 50\n",
        "embedding_matrix = load_embedding_matrix(\"zh_wiki_fasttext_300.txt\", vocab, embed_dim)\n",
        "\n",
        "model = SymptomGRUAttention(embedding_matrix, hidden_dim, len(label_to_idx)).to(device)\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def predict_symptom(text, threshold=0.35):\n",
        "    indices = text_to_indices(text, vocab, max_len)\n",
        "    input_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs, attn_weights = model(input_tensor)\n",
        "        probs = torch.softmax(outputs, dim=1)[0]\n",
        "        pred_idx = torch.argmax(probs).item()\n",
        "        confidence = probs[pred_idx].item()\n",
        "        label = idx_to_label[pred_idx] if confidence >= threshold else \"無法判斷，請再輸入一次\"\n",
        "        top3 = sorted({idx_to_label[i]: float(p) for i, p in enumerate(probs)}.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "        return {\n",
        "            \"輸入\": text,\n",
        "            \"預測結果\": label,\n",
        "            \"信心分數\": round(confidence, 4),\n",
        "            \"前三高分類別\": top3\n",
        "        }\n",
        "\n",
        "test_text = \"我想下課\"\n",
        "result = predict_symptom(test_text)\n",
        "print(f\"\\n【輸入】：{result['輸入']}\")\n",
        "print(f\"【預測結果】：{result['預測結果']}\")\n",
        "# print(\"【前三分類分數】：\")\n",
        "# for label, score in result[\"前三高分類別\"]:\n",
        "#     print(f\"{label:<10}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "從這邊以下開始run就好"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 468/468 [00:00<00:00, 2773.07 examples/s]\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\annie\\AppData\\Local\\Temp\\ipykernel_14172\\2962710255.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "c:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [265/265 12:27, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.485000</td>\n",
              "      <td>2.239037</td>\n",
              "      <td>0.446809</td>\n",
              "      <td>0.339539</td>\n",
              "      <td>0.446809</td>\n",
              "      <td>0.351095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.479700</td>\n",
              "      <td>1.324282</td>\n",
              "      <td>0.744681</td>\n",
              "      <td>0.706434</td>\n",
              "      <td>0.744681</td>\n",
              "      <td>0.696454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.047600</td>\n",
              "      <td>0.923804</td>\n",
              "      <td>0.829787</td>\n",
              "      <td>0.879433</td>\n",
              "      <td>0.829787</td>\n",
              "      <td>0.822796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.566600</td>\n",
              "      <td>0.726023</td>\n",
              "      <td>0.765957</td>\n",
              "      <td>0.848582</td>\n",
              "      <td>0.765957</td>\n",
              "      <td>0.764894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.420100</td>\n",
              "      <td>0.677718</td>\n",
              "      <td>0.829787</td>\n",
              "      <td>0.881206</td>\n",
              "      <td>0.829787</td>\n",
              "      <td>0.824316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "c:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "c:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "c:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "with open(\"sympton_dataset.json\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(raw_data)\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(sorted(df['label'].unique()))}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "df['label_id'] = df['label'].map(label2id)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
        "dataset = Dataset.from_pandas(df[[\"text\", \"label_id\"]])\n",
        "def tokenize_fn(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_fn)\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"labels\")\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "\n",
        "num_labels = len(label2id)\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-chinese\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=split_dataset[\"train\"],\n",
        "    eval_dataset=split_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"my_model/\")\n",
        "tokenizer.save_pretrained(\"my_model/\")\n",
        "\n",
        "import json\n",
        "with open(\"my_model/id2label.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(id2label, f, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "INPUT改這!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "【輸入】：我頭好痛\n",
            "【預測結果】：無法判斷\n",
            "流行性感冒     : 0.2380\n",
            "關節痛       : 0.0939\n",
            "肌肉痠痛      : 0.0786\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"my_model/\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"my_model/\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "with open(\"my_model/id2label.json\", encoding=\"utf-8\") as f:\n",
        "    id2label = json.load(f)\n",
        "id2label = {int(k): v for k, v in id2label.items()} \n",
        "def predict_symptom_new(text, threshold=0.3): # 太多無法判斷的話可以降低 threshold\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = F.softmax(logits, dim=1)[0]\n",
        "        pred_idx = torch.argmax(probs).item()\n",
        "        confidence = probs[pred_idx].item()\n",
        "        label = id2label[pred_idx] if confidence >= threshold else \"無法判斷\"\n",
        "        top3 = sorted({id2label[i]: float(p) for i, p in enumerate(probs)}.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "        return {\n",
        "            \"輸入\": text,\n",
        "            \"預測結果\": label,\n",
        "            \"信心分數\": round(confidence, 4),\n",
        "            \"前三高分類別\": top3\n",
        "        }\n",
        "\n",
        "result = predict_symptom_new(\"我頭好痛\")\n",
        "print(f\"【輸入】：{result['輸入']}\")\n",
        "print(f\"【預測結果】：{result['預測結果']}\")\n",
        "for label, score in result[\"前三高分類別\"]:\n",
        "    print(f\"{label:<10}: {score:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
